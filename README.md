# Awesome-Reasoning-MLLM

👏 Welcome to the Awesome-Reasoning-MLLM repository! This repository is a curated collection of the most influential papers, code, dataset, benchmarks, and resources about Reasoning in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to ⭐ star and fork this repository to keep up with the latest advancements and contribute to the community.


## 📒 Table of Contents

## Reinforcement Learning
* [Vision-R1] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[Paper📑]](https://arxiv.org/abs/2503.06749) [[Code🔧]](https://arxiv.org/abs/2503.06749)
* [R1-V] R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3 [[Code🔧]](https://github.com/Deep-Agent/R1-V)

## MCTS/Tree Search
* [AStar] Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking [[Paper📑]](https://arxiv.org/abs/2502.02339)
* [Mulberry] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Paper📑]](https://arxiv.org/abs/2412.18319) [[Code🔧]](https://github.com/HJYao00/Mulberry) 

<!--## Test-time Reasoning-->

## CoT
* [LLaVA-CoT] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[Paper📑]](https://arxiv.org/abs/2411.10440) [[Code🔧]](https://github.com/PKU-YuanGroup/LLaVA-CoT)

## Data
* [Mulberry 260K SFT] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Paper📑]](https://arxiv.org/abs/2412.18319) [[Code🔧]](https://github.com/HJYao00/Mulberry) 
* [LLaVA-CoT 100K SFT] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[Paper📑]](https://arxiv.org/abs/2411.10440) [[Code🔧]](https://github.com/PKU-YuanGroup/LLaVA-CoT)

## Benchmark
